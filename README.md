# Linear Regression From Scratch (NumPy)

This project implements **Linear Regression from scratch** using **Python and NumPy**, without relying on machine learning libraries like `scikit-learn`.

The goal of this project is to deeply understand:
- How linear regression works mathematically
- How gradient descent optimizes model parameters
- How weights and bias are updated during training
  
## ðŸš€ Features
- Manual implementation of:
  - Hypothesis function
  - Mean Squared Error (MSE) loss
  - Gradient Descent optimization
- Separate handling of **weights** and **bias**
- Fully vectorized operations using NumPy
- Beginner-friendly and well-commented code

## ðŸ“Œ Mathematical Background

### Hypothesis Function
y_pred = X Â· w + b

Where:
(X) = feature matrix  
(w) = weight vector  
(b) = bias  

### Loss Function (Mean Squared Error)
J(w, b) = (1/m) * Î£ (y_pred - y)Â²

### Gradients

**Derivative w.r.t. weights**
âˆ‚J/âˆ‚w = (1/m) * Xáµ€ Â· (y_pred - y)


**Derivative w.r.t. bias**
âˆ‚J/âˆ‚b = (1/m) * Î£ (y_pred - y)

### Gradient Descent Update Rules
```python
w = w - learning_rate * dw
b = b - learning_rate * db
